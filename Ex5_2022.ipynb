{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms0joNfRzag-"
      },
      "source": [
        "# Exercise Sheet 5 - Segmentation & Denoising with U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzBpDwtBzd46"
      },
      "source": [
        " * Deep Learning\n",
        " * Instructor: Constantin Pape\n",
        " * Due date: **Mon, July 04, noon**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqZXvojiz0D6"
      },
      "source": [
        "In this assignment we will use a U-Net for two different tasks:\n",
        "- semantic and instance segmentation of nuclei in microscopy images\n",
        "- denoising of natural images\n",
        "\n",
        "## Hints\n",
        "\n",
        "The goal of the first part of the exercise (segmentation) is to implement the U-Net architecture very similar to the original publication and then use it for its initial purpose, semantic segmentation. In the second part we will use the same U-Net implementation, but use it for a different task: denoising, following the ideas of noise-to-noise. \n",
        "\n",
        "To understand the background of this exercise you can:\n",
        "\n",
        "* Review the lecture (Lecture 7 for U-Net architecture, Lecture 8 for noise-to-noise)\n",
        "* Read the [U-net publication](https://arxiv.org/abs/1505.04597).\n",
        "* Read the [noise-to-noise publication](https://arxiv.org/abs/1803.04189).\n",
        "\n",
        "Note that we will implement the same ideas as in these papers, but will deviate from the implementation details and conduct different and fewer experiments.\n",
        " \n",
        "*Do not hesitate to ask questions and ideally discuss them with your fellow students and me on Rocket Chat! I will monitor the channel to provide you help if your discussions get stuck.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ4jpxuU2CqD"
      },
      "source": [
        "### IMPORTANT SUBMISSION INSTRUCTIONS\n",
        "\n",
        "- When you're done, download the notebook and rename it to \\<surname1\\>_\\<surname2\\>_\\<surname3\\>.ipynb\n",
        "- Only submit the ipynb file, no other file is required\n",
        "- Submit only once\n",
        "- The deadline is strict\n",
        "\n",
        "Implementation\n",
        "- Only change code to replace placeholders. Leave the other code as is. In addition to the python packages loaded below you are allowed to use any packages you want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aEctsMZ2a6Z"
      },
      "source": [
        "\n",
        "**Import required libraries.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "57JMWl2ZDwaq"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "import os\n",
        "import zipfile\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from matplotlib import colors\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from skimage.measure import label\n",
        "from skimage.metrics import contingency_table, peak_signal_noise_ratio\n",
        "from skimage.segmentation import find_boundaries, watershed\n",
        "from skimage.util import random_noise\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f4ScajDRtEOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03dba902-3fcd-4d82-ef0e-15ed691b71e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive and create a root folder where to save the data for this exercise\n",
        "drive.mount(\"/content/drive\")\n",
        "root_folder = \"/content/drive/MyDrive/dlforcv-ex5\"\n",
        "os.makedirs(root_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKuT7TZ7D2nM"
      },
      "source": [
        "## Segmentation\n",
        "\n",
        "In the first part of the exercise we will implement the U-Net architecture and apply it to a segmentation problem in microscopy: segmenting nuclei in fluorescence images. The data we are using is a subset of the [Kaggle Nucleus Segmentation challenge](https://www.kaggle.com/c/data-science-bowl-2018)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hstop-eGD2nm"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HHwpWifaDwbJ"
      },
      "outputs": [],
      "source": [
        "# download the data\n",
        "data_folder = os.path.join(root_folder, \"kaggle-data\")\n",
        "url = \"https://github.com/stardist/stardist/releases/download/0.1.0/dsb2018.zip\"\n",
        "if not os.path.exists(data_folder):\n",
        "    os.makedirs(data_folder)\n",
        "    tmp_path = os.path.join(data_folder, \"data.zip\")\n",
        "    with requests.get(url) as r:\n",
        "        with open(tmp_path, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "    with zipfile.ZipFile(tmp_path, \"r\") as f:\n",
        "        f.extractall(data_folder)\n",
        "    os.remove(tmp_path)\n",
        "data_folder = os.path.join(data_folder, \"dsb2018\")\n",
        "assert os.path.exists(data_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_ws-tlaDwcI"
      },
      "source": [
        "**Data exploration**\n",
        "\n",
        "The data is stored in the root folder `dsb2018` and contains the two subfolders `train` and `test` with train/test split. Each of these folders contains the folders `images` with the raw image data and `masks` with the instance masks. As a first step, we will visualize some of the images and labels from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qc4eAt0FDwcL"
      },
      "outputs": [],
      "source": [
        "image_paths = glob(os.path.join(data_folder, \"train\", \"images\", \"*.tif\"))\n",
        "image_paths.sort()\n",
        "mask_paths = glob(os.path.join(data_folder, \"train\", \"masks\", \"*.tif\"))\n",
        "mask_paths.sort()\n",
        "assert len(image_paths) == len(mask_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "juSfjLtAFIET"
      },
      "outputs": [],
      "source": [
        "# a function to generate a random color map for a label image\n",
        "def get_random_colors(labels):\n",
        "    n_labels = len(np.unique(labels)) - 1\n",
        "    cmap = [[0, 0, 0]] + np.random.rand(n_labels, 3).tolist()\n",
        "    cmap = colors.ListedColormap(cmap)\n",
        "    return cmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6nr-EMrZDwcg"
      },
      "outputs": [],
      "source": [
        "def plot_sample(image_path, mask_path):\n",
        "    image, mask = imageio.imread(image_path), imageio.imread(mask_path)\n",
        "    fig, ax = plt.subplots(1, 2)\n",
        "    ax[0].axis(\"off\")\n",
        "    ax[0].imshow(image, cmap=\"gray\")\n",
        "    # visualize the masks with random colors\n",
        "    ax[1].axis(\"off\")\n",
        "    ax[1].imshow(mask, cmap=get_random_colors(mask), interpolation=\"nearest\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgUxMoe_Dwcv"
      },
      "outputs": [],
      "source": [
        "# plot the first 4 images\n",
        "for i in range(4):\n",
        "    plot_sample(image_paths[i], mask_paths[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH9pE2uotEOL"
      },
      "source": [
        "You should see 4 images with nuclei and the corresponding colored nuclei masks. To further understand and explain the data, please answer the following QUESTIONS:\n",
        "- Do the ids assigned to the individual nuclei (= colors in the 2nd column of images) have a fixed meaning? In other words, would the segmentation change if we change shuffle these numbers, so that all pixels labeled `1` are labeled `2` and so on?\n",
        "- Can we directly use a U-Net to predict the mask ids? Why / Why not?\n",
        "- Find the unique shapes of the images in the training set.\n",
        "- Do all images have the same shape? If not, can this lead to problems? How could we deal with potentially arising problems?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mkasX89_Dwcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cdec439-5062-4a3d-a103-f483ae056702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(256, 256), (360, 360), (260, 347), (520, 696), (512, 640), (256, 320)]\n"
          ]
        }
      ],
      "source": [
        "# TODO find how many images we have in the training set\n",
        "# and find all (unique) shapes of the images in the training set\n",
        "shape = []\n",
        "for i in range(len(mask_paths)):\n",
        "  shape.append(imageio.imread(mask_paths[i]).shape)\n",
        "unique = []\n",
        "for x in shape:\n",
        "    if x not in unique:\n",
        "        unique.append(x)\n",
        "print(unique)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXMsVLAstEOM"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "- The colors do not have a fixed meaning. The numbers are just similar per nuclei and zero stands for non-nuclei\n",
        "- No, as size of the input images differ.\n",
        "- Unique shapes: [(256, 256), (360, 360), (260, 347), (520, 696), (512, 640), (256, 320)]\n",
        "- Yes as images have to have same size for U-Net. This can be solved by subsampling larger images to size of smalles images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e1z8nFHctEON"
      },
      "outputs": [],
      "source": [
        "# TODO now load the images and masks into memory, and normalize the images so that they have zero mean and unit variance\n",
        "images = [np.array(imageio.imread(path)) for path in image_paths]\n",
        "masks = [np.array(imageio.imread(path).astype(dtype = 'uint8')) for path in mask_paths]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hVldkVYqtEON"
      },
      "outputs": [],
      "source": [
        "ims_flat = np.concatenate([im.ravel() for im in images])\n",
        "mean, std = np.mean(ims_flat), np.std(ims_flat)\n",
        "images = [(im.astype(\"float32\") - mean) / std for im in images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SSdBIt3ftEOO"
      },
      "outputs": [],
      "source": [
        "# TODO make a train validation split, use every 20th image for validation\n",
        "train_images = [x for i, x in enumerate(images) if i%20 != 0]\n",
        "train_masks = [x for i, x in enumerate(masks) if i%20 != 0]\n",
        "val_images = [x for i, x in enumerate(images) if i%20 == 0]\n",
        "val_masks = [x for i, x in enumerate(masks) if i%20 == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DPAKTIATtEOO"
      },
      "outputs": [],
      "source": [
        "# finally, let's choose the appropriate torch device\n",
        "device = torch.device(\"cuda\")\n",
        "# device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH-V757fkvTJ"
      },
      "source": [
        "**Implement PyTorch dataset**\n",
        "\n",
        "As a next step, we implement a `torch.utils.data.Dataset` to have access to our data during training. As you shoulf have realized in the previous question, the images in our training set come in different sizes. In order to concatenate batches, we however need images of the same size. To solve this issue, we will subsample patches of size 256 x 256 (smallest image size) from the images in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a0xgIE_sDwdJ"
      },
      "outputs": [],
      "source": [
        "class NucleiDataset(Dataset):\n",
        "    def __init__(self, images, masks, image_transform=None, mask_transform=None, transform=None):\n",
        "        assert len(images) == len(masks)\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.images[index], self.masks[index]\n",
        "\n",
        "        # crop the images to have the shape 256 x 256, so that we can feed them into memory\n",
        "        # despite them having different sizes\n",
        "        crop_shape = (256, 256)\n",
        "        shape = image.shape\n",
        "        if shape != crop_shape:\n",
        "            assert image.ndim == mask.ndim == 2\n",
        "            crop_start = [np.random.randint(0, sh - csh) if sh != csh else 0 for sh, csh in zip(shape, crop_shape)]\n",
        "            crop = tuple(slice(cs, cs + csh) for cs, csh in zip(crop_start, crop_shape))\n",
        "            image, mask = image[crop], mask[crop]\n",
        "              \n",
        "        # apply the transforms if given\n",
        "        if self.image_transform is not None:\n",
        "            image = self.image_transform(image)\n",
        "        if self.mask_transform is not None:\n",
        "            mask = self.mask_transform(mask)\n",
        "        if self.transform is not None:\n",
        "            image, mask = self.transform(image, mask)\n",
        "        \n",
        "        # make sure we have numpy arrays and add a channel dimension for the image data\n",
        "        image, mask = np.array(image), np.array(mask)\n",
        "        if image.ndim == 2:\n",
        "            image = image[None]\n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSrKiXq2tEOQ"
      },
      "source": [
        "As discussed in the lecture, we can't learn the instance segmentation directly. So we will first start with a semantic segmentation problem and learn foreground background segmentation with the U-Net, i.e. predict for each pixel whether it belongs to a nucleus or to the background. To this end, implement a `mask_transform` that converts the instance segmentation ground-truth into a binary target and pass it to the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_transform(mask):\n",
        "  m_b = (mask != 0) * 1\n",
        "  mask_binary = np.expand_dims(m_b, axis = 0)\n",
        "  return mask_binary"
      ],
      "metadata": {
        "id": "NvyjEkWMJpCG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qS8hk7GfD2ot"
      },
      "outputs": [],
      "source": [
        "# TODO implement a transform that outputs the binary target \n",
        "# and instantiate the training dataset and validation dataset with it\n",
        "# HINT: the transform can be a function that takes the mask array as input and returns the binarized version\n",
        "# HINT: you will also need to add a channel dimension to the target\n",
        "train_dataset = NucleiDataset(train_images, train_masks, image_transform=None, mask_transform=mask_transform, transform=None)\n",
        "val_dataset = NucleiDataset(val_images, val_masks, image_transform=None, mask_transform=mask_transform, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KBNBWMXD2ov"
      },
      "outputs": [],
      "source": [
        "# TODO sample a few images from the dataset and verify that they are correct\n",
        "for i in range(9):\n",
        "  fig, ax = plt.subplots(1, 2)\n",
        "  ax[0].axis(\"off\")\n",
        "  ax[0].imshow(torch.tensor(train_dataset[i][0]).permute(2,1,0).squeeze(axis=2))\n",
        "  ax[1].axis(\"off\")\n",
        "  ax[1].imshow(torch.tensor(train_dataset[i][1]).permute(2,1,0).squeeze(axis=2))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5FPSvwotEOR"
      },
      "source": [
        "The images you sample should look similar to this (using the standard colormap for the binarized mask):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-roY4tNDweN"
      },
      "source": [
        "### Implement the U-Net\n",
        "\n",
        "Next, we will implement a U-Net architecture inspired [Ronneberger et al.](https://arxiv.org/abs/1505.04597), see the image below.\n",
        "\n",
        "IMPORTANT: unlike in the publication / image we will use SAME convolutions, so that the input shape is the same as the output shape. This will introduce some boundary artifacts, but it will make implementing the rest of this exercise much simpler.\n",
        "It also means that you don't need to crop the features passed on in the skip connections and can directly concatenate them.\n",
        "\n",
        "\n",
        "HINTS: \n",
        "- the `up-conv` used here is called [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#convtranspose2d) in pytorch.\n",
        "- you can concatenate the features coming from the skip connectins and the encoder with `torch.cat`\n",
        "- you will need `nn.ModuleList` to store the individual blocks of the encoder / decoder\n",
        "\n",
        "Before you implement the U-Net, please answer the following QUESTIONS:\n",
        "- What kind of architecture is the U-Net and what is the rationale behind this architecture?\n",
        "- What other types of architectures for semantic segmentation exist to achieve similar effects?\n",
        "- Why are the skip connections used in the U-Net?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T2X-LnmtEOR"
      },
      "source": [
        "TODO your answers here\n",
        "- Encoder-decoder architecture, learn semantic seg. in encoder, upsample semantic segmentation in decoder\n",
        "- Multiscale image pyramid methods, Deep atrous convolutions, DeepLab combines all these\n",
        "- To preserve high res information from the encoder layers that would otherwise be lost in the decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO implement the U-Net architecture\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding = 'same')\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding = 'same')\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.relu(self.conv2(self.relu(self.conv1(x))))"
      ],
      "metadata": {
        "id": "Si2djgIiJxBK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_block = Block(1, 64)\n",
        "enc_block(torch.tensor(np.expand_dims(train_dataset[0][0], axis = 0))).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TouUSk0pJyMw",
        "outputId": "2961cf0e-558b-4f19-ad62-560fdd37bad9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, chs=(1,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
        "        self.pool       = nn.MaxPool2d(2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ftrs = []\n",
        "        for block in self.enc_blocks:\n",
        "            x = block(x)\n",
        "            ftrs.append(x)\n",
        "            x = self.pool(x)\n",
        "        return ftrs"
      ],
      "metadata": {
        "id": "uqNPcv5-J0P_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "# input image\n",
        "x    = torch.tensor(train_dataset[0][0])\n",
        "ftrs = encoder(x)\n",
        "print(x.shape)\n",
        "for ftr in ftrs: print(ftr.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f30D7EaJ8pH",
        "outputId": "e71a405c-86bf-4b00-f45f-db354e3c9146"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256, 256])\n",
            "torch.Size([64, 256, 256])\n",
            "torch.Size([128, 128, 128])\n",
            "torch.Size([256, 64, 64])\n",
            "torch.Size([512, 32, 32])\n",
            "torch.Size([1024, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKlT0OzKJ_Bp",
        "outputId": "4da7edff-9e26-4612-d7d1-13304b75acb2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ftrs[-2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cq5Q12KJ_7R",
        "outputId": "58387e14-91d8-426c-db93-6f6376bbe614"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
        "        super().__init__()\n",
        "        self.chs        = chs\n",
        "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
        "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
        "        \n",
        "    def forward(self, x, encoder_features):\n",
        "        for i in range(len(self.chs)-1):\n",
        "            x        = self.upconvs[i](x)\n",
        "            x        = torch.cat([x, encoder_features[i]], dim=1)\n",
        "            x        = self.dec_blocks[i](x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "omcnupuzKCEg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "U3-itd9hDweP"
      },
      "outputs": [],
      "source": [
        "# TODO implement the U-Net architecture\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, enc_chs=(1,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1):\n",
        "        super().__init__()\n",
        "        self.encoder     = Encoder(enc_chs)\n",
        "        self.decoder     = Decoder(dec_chs)\n",
        "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_ftrs = self.encoder(x)\n",
        "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
        "        out      = self.head(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lM0HJOuXJ3p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8XBPMSZkiO3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b671781-a140-4f39-fad7-37665e795afe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (encoder): Encoder(\n",
              "    (enc_blocks): ModuleList(\n",
              "      (0): Block(\n",
              "        (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "        (relu): ReLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      )\n",
              "      (1): Block(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "        (relu): ReLU()\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      )\n",
              "      (2): Block(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "        (relu): ReLU()\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      )\n",
              "      (3): Block(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "        (relu): ReLU()\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      )\n",
              "      (4): Block(\n",
              "        (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "        (relu): ReLU()\n",
              "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      )\n",
              "    )\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (upconvs): ModuleList(\n",
              "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (dec_blocks): ModuleList(\n",
              "      (0): Block(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "        (relu): ReLU()\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      )\n",
              "      (1): Block(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "        (relu): ReLU()\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      )\n",
              "      (2): Block(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "        (relu): ReLU()\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      )\n",
              "      (3): Block(\n",
              "        (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "        (relu): ReLU()\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# TODO instantiate a uner and check that your architecture is correct by applying it to\n",
        "# an input from the train loader\n",
        "model = UNet()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(train_dataset[0][1]).permute(2,1,0).squeeze(axis=2).squeeze(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzr3t149KLfR",
        "outputId": "b804ddb1-5db2-48f3-d94b-928d652bcf80"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxRaNI9aiO3x"
      },
      "outputs": [],
      "source": [
        "# TODO display the prediction. Can you interpret the resulting image?\n",
        "pred = model(torch.tensor(np.expand_dims(train_dataset[0][0], axis = 0))).permute(1,2,3,0).squeeze(axis=3).squeeze(axis = 0)\n",
        "plt.imshow(pred.detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdnFKp25Dwd0"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now we can define the training functions, similar to the last exercise. Unlike in the last exercises we will now use [tensorboard](https://www.tensorflow.org/tensorboard) to monitor the loss, metrics and images during training and validation. We can open it directly in colab (see the cell below). Note that the data for tensorboard is stored separately from the notebook file, so if you copy the notebook somewhere else you will not see the curves in tensorboard anymore. For this exercise the tensorboard files are not part of the submission and we only include it to help you monitor the training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fT7B2fZcDwd3"
      },
      "outputs": [],
      "source": [
        "# in this cell all the training and validation functions are implemented for you already;\n",
        "# these implementations are fairly similar to the exercise where we trained a CNN on CIFAR,\n",
        "# but they now use tensorboard to log loss, metric and also images\n",
        "\n",
        "# normalize a tensor to range [0, 1] (per channel).\n",
        "# this is needed to properly display the tensor as an image in tensorboard\n",
        "def normalize(tensor):\n",
        "  eps = 1e-6\n",
        "  normed = tensor.numpy()\n",
        "  minval = normed.min(axis=(0, 2, 3), keepdims=True)\n",
        "  normed = normed - minval\n",
        "  maxval = normed.max(axis=(0, 2, 3), keepdims=True)\n",
        "  normed = normed / (maxval + eps)\n",
        "  return torch.from_numpy(normed)\n",
        "\n",
        "\n",
        "# add a tensor as image to the tensorboard\n",
        "def add_image(logger, tag, tensor, step):\n",
        "  im = tensor.detach().cpu()\n",
        "  if im.ndim == 3:\n",
        "      im = normalize(im[None])\n",
        "      logger.add_images(tag=tag, img_tensor=im, global_step=step)\n",
        "  elif im.ndim == 4 and im.shape[1] in (1, 3):\n",
        "      im = normalize(im)\n",
        "      logger.add_images(tag=tag, img_tensor=im, global_step=step)\n",
        "  elif im.ndim == 4:\n",
        "      im = normalize(im)\n",
        "      for c in range(im.shape[1]):\n",
        "          logger.add_images(tag=f\"{tag}-channel-{c}\", img_tensor=im[:, c:c+1], global_step=step)\n",
        "  else:\n",
        "      raise ValueError(f\"Expected 3 or 4d input tensor, got {im.ndim}\")\n",
        "      \n",
        "\n",
        "# run the whole training\n",
        "def run_training(\n",
        "    model, train_loader, val_loader, loss, metric, optimizer, n_epochs, logger\n",
        "):\n",
        "    epoch_len = len(train_loader)\n",
        "    step = 0\n",
        "    for epoch in tqdm.trange(n_epochs):\n",
        "        train_epoch(model, train_loader, loss, metric, optimizer, logger, step)\n",
        "        step = epoch_len * (epoch + 1)\n",
        "        validate(model, val_loader, loss, metric, logger, step)\n",
        "\n",
        "\n",
        "# train the model for one epoch\n",
        "def train_epoch(model, loader, loss, metric, optimizer, logger, step_begin):\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        pred = model(x)\n",
        "        loss_value = loss(pred, y.float())\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        if metric is not None:\n",
        "            metric_value = metric(pred, y)\n",
        "        \n",
        "        # log to the tensorboard\n",
        "        step = step_begin + i\n",
        "        # add the current loss and metric to the tensorboard\n",
        "        logger.add_scalar(tag=\"train-loss\", scalar_value=loss_value.item(), global_step=step)\n",
        "        if metric is not None:\n",
        "            logger.add_scalar(tag=\"train-metric\", scalar_value=metric_value.item(), global_step=step)\n",
        "        if step % 50 == 0:  # log images every 50 steps (every step would be too expensive)\n",
        "            add_image(logger, \"input\", x, step)\n",
        "            add_image(logger, \"target\", y, step)\n",
        "            add_image(logger, \"prediction\", pred, step)\n",
        "\n",
        "\n",
        "# validate the model\n",
        "def validate(model, loader, loss, metric, logger, step):\n",
        "    model.eval()\n",
        "    n_val = len(loader)\n",
        "    metric_value, loss_value = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss_value += loss(pred, y.float()).item()\n",
        "            if metric is not None:\n",
        "                metric_value += metric(pred, y).item()\n",
        "        \n",
        "    metric_value /= n_val\n",
        "    loss_value /= n_val\n",
        "    # log to the tensorboard\n",
        "    logger.add_scalar(tag=\"val-loss\", scalar_value=loss_value, global_step=step)\n",
        "    if metric is not None:\n",
        "        logger.add_scalar(tag=\"val-metric\", scalar_value=metric_value, global_step=step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rpSLNwstEOW"
      },
      "outputs": [],
      "source": [
        "# start the tensorboard in your notebook\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d9Wze1k-MoB"
      },
      "source": [
        "We will use (per-pixel) binary cross entropy as a loss function. Pytorch offers to different implementations of this loss: [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) and [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html).\n",
        "\n",
        "QUESTIONS:\n",
        "- What is the difference between the two different implementations?\n",
        "- Which one do we need to use here, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqZhEOs0_QNx"
      },
      "source": [
        "TODO your answers here\n",
        "- one takes values after activation (sigmoid), the other before activation\n",
        "- we use BCEWithLogits, because the U-Net does not have a activation (preferable for numeric reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "36to7hjhtEOX"
      },
      "outputs": [],
      "source": [
        "# TODO create the loss function\n",
        "loss = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLJYyUkX_mzO"
      },
      "source": [
        "Next, we want to define a suitable metric for measuring the quality of the (binary) network prediction. We will use the [dice score](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) for this, which compares the intersection of prediction and target to their union."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "yUWN9AUetEOX"
      },
      "outputs": [],
      "source": [
        "# TODO implement the dice score as a function.\n",
        "# HINTS: \n",
        "# - for later parts of this exercises, you should implement it in such a way that\n",
        "# the function can compute the dice score for input and target with multiple channels,\n",
        "# and so that it is computed independently per channel and the channel average is returned\n",
        "# - since we don't have an activation in the U-Net you need to bring the predictions in range [0, 1] using torch.sigmoid\n",
        "# - the dice score can be formulated for continuous predictions in [0, 1]; DO NOT threshold the predictions\n",
        "\n",
        "def single_dice_coef(input, target):\n",
        "    # shape of y_true and y_pred_bin: (height, width)\n",
        "    input = torch.sigmoid(input)\n",
        "    c = torch.sum(target * input) / torch.sum(target * torch.sigmoid(input))\n",
        "    if torch.sum(target * torch.sigmoid(input)) == 0:\n",
        "      c = 1\n",
        "    intersection = torch.sum(target * input)\n",
        "    if (torch.sum(target)==0) and (torch.sum(input)==0):\n",
        "        return 1\n",
        "    return (2*intersection) / (c * (torch.sum(target) + torch.sum(input)))\n",
        "\n",
        "def dice_score(input, target):\n",
        "    channel_num = target.shape[1]\n",
        "    mean_dice_channel = 0\n",
        "    for j in range(channel_num):\n",
        "      channel_dice = single_dice_coef(input[0, j, :, :], target[0, j, :, :])\n",
        "      mean_dice_channel += channel_dice/(channel_num)\n",
        "    return mean_dice_channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31vS9L6WBMF4"
      },
      "outputs": [],
      "source": [
        "# TODO check your dice implementation for an example model prediction and the corresponding target\n",
        "for i in range(20):\n",
        "  print(dice_score(model(torch.tensor(np.expand_dims(train_dataset[i][0], axis = 0))), torch.tensor(np.expand_dims(train_dataset[i][1], axis = 0))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "G8OB8CHNtEOX"
      },
      "outputs": [],
      "source": [
        "# this instantiates tensorboard logger, the logs for this model will be stored in\n",
        "# 'runs/unet-1' and the model will be called 'unet-1' in tensorbaord\n",
        "logger = SummaryWriter(\"runs/unet-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1-c_OpGAtEOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad957460-ca70-49b6-ebc8-ea7f69808871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [05:56<00:00, 35.61s/it]\n"
          ]
        }
      ],
      "source": [
        "# train the model for 10 epochs, \n",
        "# during and after the training, check the tensorboard to see how the model loss and metrics evolve,\n",
        "# and to see predictions of the training data in real time\n",
        "n_epochs = 10\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) \n",
        "val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "run_training(model, train_loader, val_loader, loss, dice_score, optimizer, n_epochs, logger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTsDojrc8QMi"
      },
      "source": [
        "## Evaluate on test data\n",
        "\n",
        "Now, we will evaluate the **instance segmentation**  derived from the model predictions on the test data. To this end, we will use [connected components](https://en.wikipedia.org/wiki/Component_(graph_theory)#Algorithms) to convert the (binarized) network predictions into an instance segmentation. We will use the implementation from [skimage](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.label).\n",
        "\n",
        "QUESTION:\n",
        "- Describe briefly what the connected components algorithm does when applied to a (binary) image. (You don't need to comment on the 'how' (implementation))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9trIRejE2Od"
      },
      "source": [
        "TODO: your answer here\n",
        "assigns unique instances to all pixels of non-touching foreground objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "gxaZPaakDwd_"
      },
      "outputs": [],
      "source": [
        "# TODO load and normalize the test images\n",
        "image_paths = glob(os.path.join(data_folder, \"test\", \"images\", \"*.tif\"))\n",
        "image_paths.sort()\n",
        "mask_paths = glob(os.path.join(data_folder, \"test\", \"masks\", \"*.tif\"))\n",
        "mask_paths.sort()\n",
        "assert len(image_paths) == len(mask_paths)\n",
        "images = [np.asarray(imageio.imread(path)) for path in image_paths]\n",
        "masks = [np.asarray(imageio.imread(path)) for path in mask_paths]\n",
        "ims_flat = np.concatenate([im.ravel() for im in images])\n",
        "images = [(im.astype(\"float32\") - mean) / std for im in images]\n",
        "test_images = images\n",
        "test_masks = masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSF3r-zVtEOZ"
      },
      "outputs": [],
      "source": [
        "# check out instance segmentation for a few test images\n",
        "counter = 0\n",
        "with torch.no_grad():\n",
        "    for im, mask in zip(test_images, test_masks):\n",
        "        if counter > 3:\n",
        "            break\n",
        "        # predict with the model and apply sigmoid to map the prediction to the range [0, 1]\n",
        "        pred = model(torch.from_numpy(im[None, None]).to(device))\n",
        "        pred = torch.sigmoid(pred).cpu().numpy().squeeze()\n",
        "        # get tbe nucleus instance segmentation by applying connected components to the binarized prediction\n",
        "        nuclei = label(pred > 0.5)\n",
        "        fig, ax = plt.subplots(1, 4, figsize=(16, 16))\n",
        "        ax[0].axis(\"off\")\n",
        "        ax[0].imshow(im, cmap=\"gray\")\n",
        "        ax[1].axis(\"off\")\n",
        "        ax[1].imshow(mask, cmap=get_random_colors(mask), interpolation=\"nearest\")\n",
        "        ax[2].axis(\"off\")\n",
        "        ax[2].imshow(pred, cmap=\"gray\")\n",
        "        ax[3].axis(\"off\")\n",
        "        ax[3].imshow(nuclei, cmap=get_random_colors(nuclei), interpolation=\"nearest\")\n",
        "        plt.show()\n",
        "        counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "EW66O9xAtEOZ"
      },
      "outputs": [],
      "source": [
        "# we also need a measure for the instance segmentation quality.\n",
        "# here, we use intersection over union (for each ground-truth object).\n",
        "# it is implemented for you already below\n",
        "\n",
        "def precision(tp, fp, fn):\n",
        "    return tp / (tp + fp) if tp > 0 else 0\n",
        "\n",
        "\n",
        "def compute_ious(seg, mask):\n",
        "    overlap = contingency_table(seg, mask).toarray()\n",
        "    n_pixels_pred = np.sum(overlap, axis=0, keepdims=True)\n",
        "    n_pixels_true = np.sum(overlap, axis=1, keepdims=True)\n",
        "    eps = 1e-7\n",
        "    ious = overlap / np.maximum(n_pixels_pred + n_pixels_true - overlap, eps)\n",
        "    # ignore matches with zero (= background)\n",
        "    ious = ious[1:, 1:]\n",
        "    n_pred, n_true = ious.shape\n",
        "    n_matched = min(n_pred, n_true)\n",
        "    return n_true, n_matched, n_pred, ious\n",
        "\n",
        "    \n",
        "def compute_tps(ious, n_matched, threshold):\n",
        "    not_trivial = n_matched > 0 and np.any(ious >= threshold)\n",
        "    if not_trivial:\n",
        "        # compute optimal matching with iou scores as tie-breaker\n",
        "        costs = -(ious >= threshold).astype(float) - ious / (2*n_matched)\n",
        "        pred_ind, true_ind = linear_sum_assignment(costs)\n",
        "        assert n_matched == len(true_ind) == len(pred_ind)\n",
        "        match_ok = ious[pred_ind, true_ind] >= threshold\n",
        "        tp = np.count_nonzero(match_ok)\n",
        "    else:\n",
        "        tp = 0\n",
        "    return tp\n",
        "\n",
        "\n",
        "def intersection_over_union(seg, mask, threshold=0.5):\n",
        "    if seg.sum() == 0:\n",
        "        return 0.0\n",
        "    n_true, n_matched, n_pred, ious = compute_ious(seg, mask)\n",
        "    tp = compute_tps(ious, n_matched, threshold)\n",
        "    fp = n_pred - tp\n",
        "    fn = n_true - tp\n",
        "    ap = precision(tp, fp, fn)\n",
        "    return ap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SRIjffAZtEOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6580c957-2651-4327-a075-41b4bd91ff72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:02<00:00, 18.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean IoU: 0.5373021580515761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute the average IOU of all the test images\n",
        "ious = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for im, mask in tqdm.tqdm(zip(test_images, test_masks), total=len(test_images)):\n",
        "        # the model can only predict images with a spatial size that is divisible by 16\n",
        "        # if it isn't we just cat a few pixels to make it fit\n",
        "        if any(sh % 16 != 0 for sh in im.shape):\n",
        "            crop = tuple(\n",
        "                slice(0, -(sh%16)) for sh in im.shape\n",
        "            )\n",
        "            im = im[crop]\n",
        "            mask = mask[crop]\n",
        "        input_ = torch.from_numpy(im[None, None]).to(device)\n",
        "        pred = model(input_)\n",
        "        pred = torch.sigmoid(pred).cpu().numpy().squeeze()\n",
        "        labels = label(pred > 0.5)\n",
        "        iou = intersection_over_union(labels, mask)\n",
        "        ious.append(iou)\n",
        "print()\n",
        "print(\"Mean IoU:\", np.mean(ious))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVuyBtjfGXI4"
      },
      "source": [
        "QUESTION:\n",
        "- Given the samples of the instance segmentations from above, what are the biggest sources of error you can identify in the model predictions / instance segmentation approach?\n",
        "- How could they be overcome?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvsf_SmHGsO-"
      },
      "source": [
        "TODO your answer here\n",
        "- nuclei (objects) are touching and cannot be separated via connected components\n",
        "- e.g. learn boundaries in addition and use them to separate the nuclei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iswz9l74DwdQ"
      },
      "source": [
        "## Train with boundary channel\n",
        "\n",
        "To avoid merges of touching nuclei, we will now add a boundary channel to the learning objective, and then use it for object separation in the instance segmentation funtion. To this end, we will train a U-Net that outputs 3 channels (background, foreground, object boundary) and use the cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in this cell all the training and validation functions are implemented for you already;\n",
        "# these implementations are fairly similar to the exercise where we trained a CNN on CIFAR,\n",
        "# but they now use tensorboard to log loss, metric and also images\n",
        "\n",
        "# normalize a tensor to range [0, 1] (per channel).\n",
        "# this is needed to properly display the tensor as an image in tensorboard\n",
        "def normalize(tensor):\n",
        "  eps = 1e-6\n",
        "  normed = tensor.numpy()\n",
        "  minval = normed.min(axis=(0, 2, 3), keepdims=True)\n",
        "  normed = normed - minval\n",
        "  maxval = normed.max(axis=(0, 2, 3), keepdims=True)\n",
        "  normed = normed / (maxval + eps)\n",
        "  return torch.from_numpy(normed)\n",
        "\n",
        "\n",
        "# add a tensor as image to the tensorboard\n",
        "def add_image(logger, tag, tensor, step):\n",
        "  im = tensor.detach().cpu()\n",
        "  if im.ndim == 3:\n",
        "      im = normalize(im[None])\n",
        "      logger.add_images(tag=tag, img_tensor=im, global_step=step)\n",
        "  elif im.ndim == 4 and im.shape[1] in (1, 3):\n",
        "      im = normalize(im)\n",
        "      logger.add_images(tag=tag, img_tensor=im, global_step=step)\n",
        "  elif im.ndim == 4:\n",
        "      im = normalize(im)\n",
        "      for c in range(im.shape[1]):\n",
        "          logger.add_images(tag=f\"{tag}-channel-{c}\", img_tensor=im[:, c:c+1], global_step=step)\n",
        "  else:\n",
        "      raise ValueError(f\"Expected 3 or 4d input tensor, got {im.ndim}\")\n",
        "      \n",
        "\n",
        "# run the whole training\n",
        "def run_training(\n",
        "    model, train_loader, val_loader, loss, metric, optimizer, n_epochs, logger\n",
        "):\n",
        "    epoch_len = len(train_loader)\n",
        "    step = 0\n",
        "    for epoch in tqdm.trange(n_epochs):\n",
        "        train_epoch(model, train_loader, loss, metric, optimizer, logger, step)\n",
        "        step = epoch_len * (epoch + 1)\n",
        "        validate(model, val_loader, loss, metric, logger, step)\n",
        "\n",
        "\n",
        "# train the model for one epoch\n",
        "def train_epoch(model, loader, loss, metric, optimizer, logger, step_begin):\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        pred = model(x)\n",
        "        loss_value = loss(pred, y)\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        if metric is not None:\n",
        "            metric_value = metric(pred, y)\n",
        "        \n",
        "        # log to the tensorboard\n",
        "        step = step_begin + i\n",
        "        # add the current loss and metric to the tensorboard\n",
        "        logger.add_scalar(tag=\"train-loss\", scalar_value=loss_value.item(), global_step=step)\n",
        "        if metric is not None:\n",
        "            logger.add_scalar(tag=\"train-metric\", scalar_value=metric_value.item(), global_step=step)\n",
        "        if step % 50 == 0:  # log images every 50 steps (every step would be too expensive)\n",
        "            add_image(logger, \"input\", x, step)\n",
        "            add_image(logger, \"target\", y, step)\n",
        "            add_image(logger, \"prediction\", pred, step)\n",
        "\n",
        "\n",
        "# validate the model\n",
        "def validate(model, loader, loss, metric, logger, step):\n",
        "    model.eval()\n",
        "    n_val = len(loader)\n",
        "    metric_value, loss_value = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss_value += loss(pred, y).item()\n",
        "            if metric is not None:\n",
        "                metric_value += metric(pred, y).item()\n",
        "        \n",
        "    metric_value /= n_val\n",
        "    loss_value /= n_val\n",
        "    # log to the tensorboard\n",
        "    logger.add_scalar(tag=\"val-loss\", scalar_value=loss_value, global_step=step)\n",
        "    if metric is not None:\n",
        "        logger.add_scalar(tag=\"val-metric\", scalar_value=metric_value, global_step=step)"
      ],
      "metadata": {
        "id": "BorGBV5jL759"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "F3yovSuCtEOb"
      },
      "outputs": [],
      "source": [
        "# TODO implement a transform that outputs an image with 3 values:\n",
        "# 0 for backgorund, 1 for foreground (= nucleus) and 2 for boundary (boundary pixel between nucleus and background or between 2 nuclei)\n",
        "# HINT: you can use skimage.segmentation.find_boundaries (already imported) to determine the boundary pixels\n",
        "def label_transform(mask):\n",
        "  mask = mask_transform(mask)\n",
        "  mask_b = find_boundaries(mask) * 2\n",
        "  mask = mask + mask_b\n",
        "  mask[mask>2] = 2\n",
        "  return mask.squeeze(axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0mJyxnt-ZZEz"
      },
      "outputs": [],
      "source": [
        "# instantiate the training and validation datasets with the new label transform\n",
        "train_dataset = NucleiDataset(\n",
        "    train_images, train_masks, mask_transform=label_transform\n",
        ")\n",
        "val_dataset = NucleiDataset(\n",
        "    val_images, val_masks, mask_transform=label_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3wvubNWtEOc"
      },
      "outputs": [],
      "source": [
        "# visualize the new label transform and make sure it's correct\n",
        "counter = 0\n",
        "for im, target in train_dataset:\n",
        "    if counter > 3:\n",
        "        break\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(16, 16))\n",
        "    ax[0].axis(\"off\")\n",
        "    ax[0].imshow(im[0], cmap=\"gray\")\n",
        "    ax[1].axis(\"off\")\n",
        "    ax[1].imshow(target)\n",
        "    plt.show()\n",
        "    counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "cSX8UkT-tEOd"
      },
      "outputs": [],
      "source": [
        "# TODO instantiate the new unet and loss function\n",
        "model = UNet(enc_chs=(1,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=3)\n",
        "model.to(device)\n",
        "loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DeVYkiejtEOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6076cca9-ce9e-444f-f97b-f9a001da7b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [06:10<00:00, 37.04s/it]\n"
          ]
        }
      ],
      "source": [
        "# train the new U-Net for 10 epochs\n",
        "# (we don't use a metric here, since the target (with class labels 0, 1, 2) and prediction (one-hot encoding) have different representations\n",
        "n_epochs = 10\n",
        "logger = SummaryWriter(\"runs/unet-2\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) \n",
        "val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "metric = None\n",
        "run_training(model, train_loader, val_loader, loss, metric, optimizer, n_epochs, logger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLCB3ETltEOd"
      },
      "outputs": [],
      "source": [
        "# TODO write instance segmentation function where you first subtract the  boundary from the foreground prediction,\n",
        "# and then apply connected components to the result in order to get the instances\n",
        "# the parameter 'threshold' should determine at which threshold the result is binarized before applying connected components \n",
        "# HINT: you can use skimage.segmentation.watershed (already implemented) afterwards to grow the segmentation back so that it\n",
        "# covers the full prediction while keeping the instance labels\n",
        "def instance_segmentation(foreground_prediction, boundary_prediction, threshold=0.5):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5opFzC7BtEOe"
      },
      "outputs": [],
      "source": [
        "# TODO check the prediction results and instance segmentation for a few images\n",
        "# make sure your instance segmentation implementation is correct\n",
        "# HINT: you need to apply a softmax to the network predictions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijQqjb1TLKHq"
      },
      "outputs": [],
      "source": [
        "# TODO use the validation set to find a good value for the 'threshold' parameter in the instance_segmentation function\n",
        "best_threshold = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFJVVjBgtEOe"
      },
      "outputs": [],
      "source": [
        "# Compute the average IOU of all the test images\n",
        "ious = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for im, mask in tqdm.tqdm(zip(test_images, test_masks), total=len(test_images)):\n",
        "        # \n",
        "        if any(sh % 16 != 0 for sh in im.shape):\n",
        "            crop = tuple(\n",
        "                slice(0, -(sh%16)) for sh in im.shape\n",
        "            )\n",
        "            im = im[crop]\n",
        "            mask = mask[crop]\n",
        "        \n",
        "        input_ = torch.from_numpy(im[None, None]).to(device)\n",
        "        pred = model(input_)\n",
        "        pred = torch.softmax(pred, dim=1).cpu().numpy().squeeze()\n",
        "        assert pred.shape[0] == 3\n",
        "        nuclei = instance_segmentation(pred[1], pred[2])\n",
        "\n",
        "        iou = intersection_over_union(nuclei, mask, best_threshold)\n",
        "        ious.append(iou)\n",
        "print()\n",
        "print(np.mean(ious))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T92GcWv-Mbl5"
      },
      "source": [
        "QUESTIONS:\n",
        "- Did training with the boundaries and using them in the instance segmentation improve the results?\n",
        "- Can you think of ways to improve the loss function for this learning task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29djWBqJM5B2"
      },
      "source": [
        "TODO your answers here\n",
        "- Yes, should have improved it, but depends a bit on the learning process...\n",
        "- Boundary target is quite unbalanced, use a balanced loss function, e.g. Dice Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vyoYLsYDwdY"
      },
      "source": [
        "## [Optional] Try variations of the UNet architecture\n",
        "\n",
        "- add BatchNorm\n",
        "- replace TransposedConvs with bilinear upsampling and 1x1 convs\n",
        "- use residual blocks instead of the normal convolutional blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI6b6ou6DwfZ"
      },
      "outputs": [],
      "source": [
        "# TODO implement and compare u-net architecture variations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ZPvRojDweF"
      },
      "source": [
        "## [Optional] Train with data augmentation\n",
        "\n",
        "- rotations & flips\n",
        "- noise augmentations (add gaussian noise and/or poisson noise)\n",
        "- crop and resize\n",
        "\n",
        "Hint: use the `image_transform` (noise augmentations) and `transform` (geometrical augmentations) argument of the `NucleiDataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RBIeShPiO3z"
      },
      "outputs": [],
      "source": [
        "# TODO implement data augmentations and compare with training without augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxHRN-_jDwfW"
      },
      "source": [
        "# Denoising with Noise2Noise\n",
        "\n",
        "Here, we implement [Noise2Noise](https://arxiv.org/abs/1803.04189), which is a method to learn image denoising **without** ground-truth, by using a different noisy image as target. Note that we are re-using the same architecture as before, which is a bit different from the one used in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUn7PLMBiO3z"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "We use the [VSDR dataset](https://cv.snu.ac.kr/research/VDSR), which contains 'clean' (i.e. without noise) natural images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "PyOXbJAytEOj"
      },
      "outputs": [],
      "source": [
        "# Download the VSDR data\n",
        "def download(url, output_folder):\n",
        "    if os.path.exists(output_folder):\n",
        "        return\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    tmp_path = os.path.join(output_folder, \"data.zip\")\n",
        "    with requests.get(url) as r:\n",
        "        with open(tmp_path, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "    with zipfile.ZipFile(tmp_path, \"r\") as f:\n",
        "        f.extractall(output_folder)\n",
        "    os.remove(tmp_path)\n",
        "\n",
        "train_url = \"https://cv.snu.ac.kr/research/VDSR/train_data.zip\"\n",
        "test_url = \"https://cv.snu.ac.kr/research/VDSR/test_data.zip\"\n",
        "\n",
        "vsdr_train = os.path.join(root_folder, \"vsdr_train\")\n",
        "download(train_url, vsdr_train)\n",
        "vsdr_test = os.path.join(root_folder, \"vsdr_test\")\n",
        "download(test_url, vsdr_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del vsdr_train"
      ],
      "metadata": {
        "id": "XDfmg0wGXxVT"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "QUFOcqiatEOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2f7824-0224-4a37-d811-76c1e8ba8e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(769, 1024, 3), (344, 228, 3), (576, 1024, 3), (321, 481, 3), (665, 1024, 3), (480, 500, 3), (362, 500, 3), (689, 1024, 3), (644, 1024, 3), (1024, 685, 3), (391, 586, 3), (276, 276, 3), (819, 1024, 3), (672, 1024, 3), (737, 1024, 3), (692, 1024, 3), (1024, 742, 3), (694, 1024, 3), (1024, 683, 3), (1200, 798, 3), (576, 720, 3), (1024, 700, 3), (1037, 778, 3), (1024, 768, 3), (805, 1024, 3), (616, 1024, 3), (681, 1024, 3), (512, 768, 3), (768, 1024, 3), (512, 512, 3), (288, 352, 3), (256, 256, 3), (512, 512), (683, 1024, 3), (1024, 1024, 3), (575, 1024, 3), (766, 1024, 3), (481, 321, 3), (963, 1280, 3), (660, 1024, 3), (835, 1024, 3), (727, 1024, 3), (656, 529, 3), (1024, 1005, 3), (773, 1024, 3), (827, 1200, 3), (980, 1000, 3), (732, 1024, 3), (797, 1024, 3), (864, 1024, 3), (673, 1024, 3), (1024, 692, 3), (695, 1024, 3), (1024, 567, 3), (280, 280, 3), (361, 250, 3), (656, 1024, 3), (676, 1024, 3), (680, 1024, 3), (678, 1024, 3), (682, 1024, 3), (765, 1024, 3), (288, 288, 3)]\n"
          ]
        }
      ],
      "source": [
        "# check the unique image sizes in the training data\n",
        "image_exts = (\".jpeg\", \".jpg\", \".png\", \".bmp\")\n",
        "all_image_paths = [\n",
        "    p for p in Path(vsdr_test).glob(\"**/*\") if p.suffix.lower() in image_exts\n",
        "]\n",
        "shapes = []\n",
        "for p in all_image_paths:\n",
        "    shapes.append(imageio.imread(p).shape)\n",
        "unique_shapes = list(set(shapes))\n",
        "print(unique_shapes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "EDoPTuuQtEOl"
      },
      "outputs": [],
      "source": [
        "# vsdr dataset that can deal with all the different data sizes in the training and test data\n",
        "class VsdrDataset(Dataset):\n",
        "    def __init__(self, root_dir, noise_transform):\n",
        "        image_exts = (\".jpeg\", \".jpg\", \".png\", \".bmp\")\n",
        "        self.image_paths = [\n",
        "            p for p in Path(root_dir).glob(\"**/*\") if p.suffix.lower() in image_exts\n",
        "        ]\n",
        "        self.noise_transform = noise_transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        im = np.array(imageio.imread(self.image_paths[index]))\n",
        "        if im.ndim == 2:\n",
        "            im = np.concatenate([im[..., None]] * 3, axis=2)\n",
        "        if im.shape[-1] == 1:\n",
        "            im = np.concatenate([im] * 3, axis=2)\n",
        "        target_shape = (256, 256)\n",
        "        # first, pad the images if they are smaller than the target crop shape\n",
        "        if any(sh < tsh for sh, tsh in zip(im.shape[:-1], target_shape)):\n",
        "            padding = tuple((0, tsh - sh if sh < tsh else 0) for sh, tsh in zip(im.shape[:-1], target_shape))\n",
        "            padding = padding + ((0, 0),)\n",
        "            im = np.pad(im, padding, mode=\"reflect\")\n",
        "        # then crop tham if they are too large\n",
        "        if any(sh > tsh for sh, tsh in zip(im.shape[:-1], target_shape)):\n",
        "            crop_start = [np.random.randint(0, sh - tsh) if sh != tsh else 0 for sh, tsh in zip(im.shape[:-1], target_shape)]\n",
        "            crop = tuple(slice(cs, cs + tsh) for cs, tsh in zip(crop_start, target_shape))\n",
        "            im = im[crop]\n",
        "        assert im.shape == (256, 256, 3), f\"{im.shape}\"\n",
        "        \n",
        "        # normalize the image to range [0, 1] (per channel)\n",
        "        im = im.astype(\"float32\")\n",
        "        im -= im.min(axis=(0, 1), keepdims=True)\n",
        "        im /= im.max(axis=(0, 1), keepdims=True)\n",
        "        # bring image in range [-1, 1]\n",
        "        im = 2 * im - 1\n",
        "        \n",
        "        # transform to channel first order\n",
        "        im = im.transpose((2, 0, 1))\n",
        "        # apply the noise transformation\n",
        "        return self.noise_transform(im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "IX1GUBwQYggE"
      },
      "outputs": [],
      "source": [
        "# go from range [-1, 1] to [0, 255] and uint 8\n",
        "# and go to channel last order\n",
        "def to_display(im):\n",
        "    im = (((im + 1) / 2) * 255).astype(\"uint8\")\n",
        "    return im.transpose((1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB_Uw6V2tEOl"
      },
      "outputs": [],
      "source": [
        "# sample a few images\n",
        "check_dataset = VsdrDataset(vsdr_test, lambda x: x)\n",
        "fig, ax = plt.subplots(4, 4, figsize=(16, 16))\n",
        "for ii in range(16):\n",
        "    im = check_dataset[ii]\n",
        "    ax[ii % 4, ii // 4].axis(\"off\")\n",
        "    ax[ii % 4, ii // 4].imshow(to_display(im))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owPARSsUtEOm"
      },
      "source": [
        "## Train a network with clean targets\n",
        "\n",
        "As the first baseline, we will train a network with clean target data (i.e. without noise added to it). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "uprF3UhKtEOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e05ce44-32e6-404a-9dd2-9998d64fc189"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MSELoss()"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "# instatiate the model and loss function (Mean Squared Error)\n",
        "model = UNet(enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1)\n",
        "model.to(device)\n",
        "loss = nn.MSELoss()\n",
        "loss.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "047zI_jEtEOm"
      },
      "outputs": [],
      "source": [
        "# we will use the PSNR (peak signal to noise ratio) as metric to evaluate the predictions.\n",
        "# you can read up on it and the implementation we are using here:\n",
        "# https://www.ni.com/en-us/innovations/white-papers/11/peak-signal-to-noise-ratio-as-an-image-quality-metric.html\n",
        "# (the higher the PSNR, the better)\n",
        "\n",
        "def to_uint8(im):\n",
        "    return torch.clamp((im + 0.5) * 255.0 + 0.5, 0, 255).type(torch.uint8)\n",
        "\n",
        "\n",
        "class PSNR(nn.Module):\n",
        "    def __call__(self, x, y):\n",
        "        x, y = to_uint8(x), to_uint8(y)\n",
        "        x, y = x.detach().cpu().numpy(), y.detach().cpu().numpy()\n",
        "        return peak_signal_noise_ratio(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUsnzzsfbCCY"
      },
      "source": [
        "QUESTION:\n",
        "- What does the PSNR metric measuere?\n",
        "- Why is it used instead of 'just' computing the pixel wise error (MSE)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6sHp7SCbCQH"
      },
      "source": [
        "TODO your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "VS27dXkdtEOn"
      },
      "outputs": [],
      "source": [
        "# TODO implement a function that adds additive gaussian noise to the input\n",
        "# the var argument should correspond to the variance of the gaussian\n",
        "def additive_gaussian_noise(x, var):\n",
        "  return x + np.random.normal(0,np.sqrt(var),x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlWDwrTttEOn"
      },
      "outputs": [],
      "source": [
        "# TODO instantiate a vsdr dataset corresponding to the vsdr_train folder \n",
        "# with a noise transformation that returns a corrupted image (as network input)\n",
        "# and a clean image (as target)\n",
        "# use a variance of 0.1\n",
        "# split the dataset into a train (90% of the data) and validationd dataset (10%)\n",
        "# HINT: you can use torch.utils.data.random_split to split a torch dataset\n",
        "\n",
        "vsdr_train_ds =\n",
        "vsdr_val_ds ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOfan18BtEOn"
      },
      "outputs": [],
      "source": [
        "# sample a few images from the train dataset\n",
        "fig, ax = plt.subplots(3, 2, figsize=(16, 16))\n",
        "for ii in range(3):\n",
        "    noisy, clean = vsdr_train_ds[ii]\n",
        "    # go back to uint8\n",
        "    ax[ii, 0].axis(\"off\")\n",
        "    ax[ii, 0].imshow(to_display(noisy))\n",
        "    ax[ii, 1].axis(\"off\")\n",
        "    ax[ii, 1].imshow(to_display(clean))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF8InzHOtEOo"
      },
      "outputs": [],
      "source": [
        "# train the denoising network\n",
        "n_epochs = 10\n",
        "logger = SummaryWriter(\"runs/noise2noise-1\")\n",
        "train_loader = DataLoader(vsdr_train_ds, batch_size=1, shuffle=True) \n",
        "val_loader = DataLoader(vsdr_val_ds, batch_size=1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "metric = PSNR()\n",
        "run_training(model, train_loader, val_loader, loss, metric, optimizer, n_epochs, logger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1vcIoNktEOo"
      },
      "outputs": [],
      "source": [
        "# create the test dataset\n",
        "vsdr_test_ds = VsdrDataset(\n",
        "    vsdr_test,\n",
        "    noise_transform=lambda x: (additive_gaussian_noise(x, var=0.1), x)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sXibWN7tEOo"
      },
      "outputs": [],
      "source": [
        "# TODO display some results on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt-oUdgDtEOp"
      },
      "source": [
        "## Train a network with noisy targets (Gaussian noise)\n",
        "\n",
        "Now, we train a network from noisy inputs AND noisy targets. And compare its results to the previous denoising network learned on clean targets.\n",
        "\n",
        "QUESTION:\n",
        "- Why can the model learn denoising even though we also have a noisy target?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAv6kyG_alyB"
      },
      "source": [
        "TODO your answer here\n",
        "- uncorrelated noise with zero mean does not affect the MSE loss (in limit to infinity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV_btkqvtEOq"
      },
      "outputs": [],
      "source": [
        "# instantiate new model and loss\n",
        "model = UNet(in_channels=3, out_channels=3)\n",
        "model.to(device)\n",
        "loss = nn.MSELoss()\n",
        "loss.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZaAPMcPtEOq"
      },
      "outputs": [],
      "source": [
        "# TODO create dataset with corrupted input and target\n",
        "# use same variance as before (0.1)\n",
        "vsdr_train_ds =\n",
        "vsdr_val_ds ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPT68QcztEOq"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "n_epochs = 10\n",
        "logger = SummaryWriter(\"runs/noise2noise-2\")\n",
        "train_loader = DataLoader(vsdr_train_ds, batch_size=1, shuffle=True) \n",
        "val_loader = DataLoader(vsdr_val_ds, batch_size=1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "metric = PSNR()\n",
        "run_training(model, train_loader, val_loader, loss, metric, optimizer, n_epochs, logger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJeSUJtdtEOq"
      },
      "outputs": [],
      "source": [
        "# TODO show a few predictions for the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhkIuCYRtEOr"
      },
      "outputs": [],
      "source": [
        "# TODO compute the PSNR for the whole test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEM4zGP5c6fc"
      },
      "source": [
        "QUESTIONS:\n",
        "- Compare the result between the network trained with clean targets and with noisy targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzUpXT7Vc6mr"
      },
      "source": [
        "TODO your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqeie9E7tEOr"
      },
      "source": [
        "## [Optional] Train a network with noisy targets (Poisson noise)\n",
        "\n",
        "Use poisson noise instead of gaussian noise and compare to both the network trained with clean target and the network with the target corrupted by gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w6WEbf8tEOs"
      },
      "outputs": [],
      "source": [
        "# TODO run training with poisson noise and compare with previous results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UVjjYlrdJKX"
      },
      "source": [
        "## [Optional] Use U-Net with BatchNorm\n",
        "\n",
        "Repeat the exercise with a U-Net with BatchNorm. Does this improve the results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7hn6OD-dJWQ"
      },
      "outputs": [],
      "source": [
        "# TODO use a U-Net architecture with BatchNorm and compare with previous results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Ex5_2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}